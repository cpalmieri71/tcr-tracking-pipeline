#!/usr/bin/env python3
"""
02_dynamics_gbm_ou.py
====================

Dynamic comparison of GBM vs OU models (with optional OU refinement)
on clonotype trajectories derived from the noise/QC/denoising pipeline.

This script is notebook-free. It reads the outputs generated by
`01_noise_pipeline_full_with_freq_v2.py` (or equivalent) and produces:

- a filtered set of included clonotypes (observable in >= N timepoints;
  N = 0 disables filtering)
- long-format trajectory tables (frequency and log-frequency with metadata)
- per-subject GBM fits (in log-frequency space)
- per-subject OU fits (in log-frequency space)
- model comparison metrics (log-likelihood, AIC, BIC)
- optional: OU refinement stratified by frequency bins, with bootstrap CIs

Default modeling assumptions (configurable via CLI options):
- Modeled variable: x(t) = log(freq), with preferred frequency = freq_geo
- Only timepoints with observable=True are used (no dropout imputation),
  unless --include-non-observable is set
- Per-subject fits aggregate transitions across all included clonotypes
- Time increment dt is inferred from differences in the `time` column
  (e.g. weeks)

Requirements
------------
- Python 3.9+
- numpy, pandas
- scipy (required for OU maximum-likelihood estimation)

Expected inputs (inside --results-dir)
--------------------------------------
- trajectory_observability_{null}_alpha{alpha}.csv
- trajectory_observability_counts_{null}_alpha{alpha}.csv

where {null} ∈ {clone, subject, global}

Examples
--------
1) Fit and compare models, requiring ≥2 observable timepoints (default):
   python 02_dynamics_gbm_ou.py --results-dir results --null subject --alpha 0.01

2) Disable clonotype filtering (N=0):
   python 02_dynamics_gbm_ou.py --results-dir results --null subject --alpha 0.01 --min-times-observable 0

3) Specify frequency column explicitly:
   python 02_dynamics_gbm_ou.py --results-dir results --null subject --alpha 0.01 --freq-col freq_geo

4) Use all (t1, t2) observable pairs instead of consecutive ones:
   python 02_dynamics_gbm_ou.py --results-dir results --null subject --alpha 0.01 --pairing all

5) OU refinement by log10-frequency bins with 200 bootstrap samples:
   python 02_dynamics_gbm_ou.py --results-dir results --null subject --alpha 0.01 \
       --refine-ou --n-bins 6 --bootstrap 200

Outputs (in --out-dir, default: <results-dir>/dynamics)
-------------------------------------------------------
- included_clones_min{N}.csv
- trajectories_long_min{N}.csv
- gbm_fit_by_subject.csv
- ou_fit_by_subject.csv
- model_comparison_by_subject.csv
- (if --refine-ou) ou_refined_by_subject_bin.csv
- (if --refine-ou and --bootstrap>0) ou_refined_by_subject_bin_bootstrap_ci.csv
"""


from __future__ import annotations

import argparse
import math
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, List, Optional, Tuple

import numpy as np
import pandas as pd

# SciPy è usato solo per OU MLE (minimize)
try:
    from scipy.optimize import minimize
except Exception:
    minimize = None


# -----------------------------
# Helpers
# -----------------------------
def ensure_dir(p: Path) -> Path:
    p.mkdir(parents=True, exist_ok=True)
    return p


def infer_clone_col(df: pd.DataFrame) -> str:
    candidates = ["aaSeqCDR3", "cloneId", "cloneID", "clonotype", "cdr3"]
    for c in candidates:
        if c in df.columns:
            return c
    raise ValueError(f"Non trovo colonna clonotipo. Colonne disponibili: {list(df.columns)}")


def infer_freq_col(df: pd.DataFrame) -> str:
    prefs = [
        "freq_geo", "freq_geo_y", "freq_geo_x",
        "freq_mean",
        "freq_rep1", "freq_rep1_y", "freq_rep1_x",
        "freq_rep2", "freq_rep2_y", "freq_rep2_x",
        "readFraction", "freq"
    ]
    for c in prefs:
        if c in df.columns:
            return c
    raise ValueError(
        "Non trovo una colonna frequenza (freq_geo/freq_rep1/readFraction/freq...). "
        f"Colonne disponibili: {list(df.columns)}"
    )


def safe_log(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:
    return np.log(np.clip(x, eps, None))


def all_pairs_indices(n: int) -> Iterable[Tuple[int, int]]:
    for i in range(n - 1):
        for j in range(i + 1, n):
            yield i, j


def load_freq_from_denoised(results_dir: Path, null_choice: str, alpha: float) -> pd.DataFrame:
    """
    Carica per_clone_denoised_{null}_alpha{alpha}.csv e restituisce una tabella
    (subject, time, clone, freq) con freq preferita (freq_geo > freq_rep1 > freq_rep2 > readFraction).
    Gestisce anche colonne duplicate con suffisso _x/_y.
    """
    den_path = results_dir / f"per_clone_denoised_{null_choice}_alpha{alpha}.csv"
    if not den_path.exists():
        raise FileNotFoundError(
            f"Manca la colonna frequenza nei trajectory_* e non trovo il denoised file: {den_path}"
        )
    den = pd.read_csv(den_path)

    clone_col = infer_clone_col(den)
    required = {"subject", "time", clone_col}
    missing = required - set(den.columns)
    if missing:
        raise ValueError(f"{den_path.name}: colonne mancanti {missing}")

    freq_candidates = [
        "freq_geo", "freq_geo_y", "freq_geo_x",
        "freq_rep1", "freq_rep1_y", "freq_rep1_x",
        "freq_rep2", "freq_rep2_y", "freq_rep2_x",
        "readFraction", "freq"
    ]
    freq_col = next((c for c in freq_candidates if c in den.columns), None)
    if freq_col is None:
        raise ValueError(
            f"{den_path.name}: non trovo una colonna frequenza tra {freq_candidates}. "
            f"Colonne: {list(den.columns)}"
        )

    out = den[["subject", "time", clone_col, freq_col]].copy()
    out = out.rename(columns={clone_col: "clone", freq_col: "freq"})
    out = (out.dropna(subset=["freq"])
              .groupby(["subject", "time", "clone"], as_index=False)["freq"].max())
    return out


# -----------------------------
# Transition extraction
# -----------------------------
@dataclass
class TransitionSet:
    dt: np.ndarray
    x0: np.ndarray
    x1: np.ndarray
    subject: int

    @property
    def n(self) -> int:
        return int(self.dt.size)


def extract_transitions_for_subject(
    traj: pd.DataFrame,
    subject: int,
    clone_col: str,
    freq_col: str,
    time_col: str,
    observable_col: str,
    pairing: str = "consecutive",
    use_only_observable: bool = True,
) -> TransitionSet:
    df = traj[traj["subject"] == subject].copy()
    if df.empty:
        return TransitionSet(dt=np.array([]), x0=np.array([]), x1=np.array([]), subject=subject)

    if use_only_observable:
        df = df[df[observable_col] == True].copy()

    df = df.dropna(subset=[freq_col, time_col, clone_col])
    df = df[df[freq_col] > 0].copy()
    df = df.sort_values([clone_col, time_col])

    dts, x0s, x1s = [], [], []
    for _, g in df.groupby(clone_col):
        t = g[time_col].to_numpy(dtype=float)
        f = g[freq_col].to_numpy(dtype=float)
        if t.size < 2:
            continue
        x = safe_log(f)

        if pairing == "consecutive":
            for i in range(t.size - 1):
                dt = float(t[i + 1] - t[i])
                if dt <= 0:
                    continue
                dts.append(dt)
                x0s.append(float(x[i]))
                x1s.append(float(x[i + 1]))
        elif pairing == "all":
            for i, j in all_pairs_indices(t.size):
                dt = float(t[j] - t[i])
                if dt <= 0:
                    continue
                dts.append(dt)
                x0s.append(float(x[i]))
                x1s.append(float(x[j]))
        else:
            raise ValueError("pairing deve essere 'consecutive' o 'all'")

    return TransitionSet(
        dt=np.asarray(dts, dtype=float),
        x0=np.asarray(x0s, dtype=float),
        x1=np.asarray(x1s, dtype=float),
        subject=subject,
    )


# -----------------------------
# GBM MLE in log-space
# dx ~ N(mu dt, sigma^2 dt)
# -----------------------------
@dataclass
class GBMFit:
    mu: float
    sigma: float
    loglik: float
    n: int


def fit_gbm(trans: TransitionSet) -> Optional[GBMFit]:
    if trans.n < 2:
        return None

    dx = trans.x1 - trans.x0
    dt = trans.dt
    dt_sum = float(dt.sum())
    if dt_sum <= 0:
        return None

    mu = float(dx.sum() / dt_sum)
    resid = dx - mu * dt
    sigma2 = float((resid**2).sum() / dt_sum)
    sigma2 = max(sigma2, 1e-12)
    sigma = float(math.sqrt(sigma2))

    ll = float(
        (-0.5 * np.log(2 * np.pi * sigma2 * dt) - (resid**2) / (2 * sigma2 * dt)).sum()
    )
    return GBMFit(mu=mu, sigma=sigma, loglik=ll, n=trans.n)


# -----------------------------
# OU MLE in log-space (exact transition)
# -----------------------------
@dataclass
class OUFit:
    lam: float
    m: float
    sigma: float
    loglik: float
    n: int
    success: bool
    message: str


def ou_negloglik(params: np.ndarray, x0: np.ndarray, x1: np.ndarray, dt: np.ndarray) -> float:
    lam, m, sigma = float(params[0]), float(params[1]), float(params[2])
    lam = max(lam, 1e-12)
    sigma = max(sigma, 1e-12)

    e = np.exp(-lam * dt)
    mean = m + (x0 - m) * e
    var = (sigma**2 / (2 * lam)) * (1 - np.exp(-2 * lam * dt))
    var = np.clip(var, 1e-12, None)

    resid = x1 - mean
    nll = 0.5 * np.sum(np.log(2 * np.pi * var) + (resid**2) / var)
    return float(nll)


def fit_ou(trans: TransitionSet, maxiter: int = 2000) -> Optional[OUFit]:
    if trans.n < 3:
        return None
    if minimize is None:
        raise RuntimeError("SciPy non disponibile (scipy.optimize.minimize). Installa con: pip install scipy")

    x0, x1, dt = trans.x0, trans.x1, trans.dt
    dx = x1 - x0

    m0 = float(np.mean(np.concatenate([x0, x1])))
    dt_med = float(np.median(dt)) if dt.size else 1.0
    lam0 = float(1.0 / max(dt_med, 1e-6))
    sigma0 = float(np.std(dx) / math.sqrt(max(np.mean(dt), 1e-6)))
    sigma0 = max(sigma0, 1e-3)

    x_init = np.array([lam0, m0, sigma0], dtype=float)
    bounds = [(1e-8, None), (None, None), (1e-8, None)]  # lam>0, sigma>0

    res = minimize(
        ou_negloglik,
        x0=x_init,
        args=(x0, x1, dt),
        method="L-BFGS-B",
        bounds=bounds,
        options={"maxiter": maxiter},
    )

    lam, m, sigma = res.x
    lam = float(max(lam, 1e-12))
    sigma = float(max(sigma, 1e-12))
    ll = -float(res.fun)

    return OUFit(
        lam=lam,
        m=float(m),
        sigma=sigma,
        loglik=ll,
        n=trans.n,
        success=bool(res.success),
        message=str(res.message),
    )


# -----------------------------
# Model comparison metrics
# -----------------------------
def aic(loglik: float, k: int) -> float:
    return float(2 * k - 2 * loglik)


def bic(loglik: float, k: int, n: int) -> float:
    n = max(int(n), 1)
    return float(k * math.log(n) - 2 * loglik)


# -----------------------------
# OU refinement by frequency bins
# -----------------------------
def assign_bins(values: np.ndarray, n_bins: int, edges: Optional[List[float]] = None) -> np.ndarray:
    if edges is not None and len(edges) >= 2:
        return np.digitize(values, bins=np.array(edges), right=False)
    qs = np.linspace(0, 1, n_bins + 1)
    cuts = np.quantile(values, qs)
    cuts = np.unique(cuts)
    if cuts.size < 3:
        cuts = np.linspace(values.min(), values.max(), n_bins + 1)
    return np.digitize(values, bins=cuts[1:-1], right=False) + 1


def bootstrap_ou(
    x0: np.ndarray, x1: np.ndarray, dt: np.ndarray,
    n_boot: int, maxiter: int, seed: int
) -> pd.DataFrame:
    rng = np.random.default_rng(seed)
    n = x0.size
    out = []
    for b in range(n_boot):
        idx = rng.integers(0, n, size=n)
        trans = TransitionSet(dt=dt[idx], x0=x0[idx], x1=x1[idx], subject=-1)
        fit = fit_ou(trans, maxiter=maxiter)
        if fit is None:
            continue
        out.append({
            "boot": b, "lam": fit.lam, "m": fit.m, "sigma": fit.sigma,
            "loglik": fit.loglik, "success": fit.success
        })
    return pd.DataFrame(out)


# -----------------------------
# Main
# -----------------------------
def build_argparser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(
        formatter_class=argparse.RawTextHelpFormatter,
        description=(
            "Confronto dinamico GBM vs OU (e rifinitura OU) su traiettorie clonotipiche.\n\n"
            "Legge gli output del pipeline noise/QC e produce fit per soggetto + confronto.\n"
        ),
    )

    p.add_argument("--results-dir", type=Path, required=True,
                   help="Cartella results prodotta dal pipeline noise (es. ./results)")
    p.add_argument("--out-dir", type=Path, default=None,
                   help="Cartella output dinamica (default: <results-dir>/dynamics)")
    p.add_argument("--null", dest="null_choice", choices=["clone", "subject", "global"], required=True,
                   help="Scelta null usata per generare trajectory_* (deve corrispondere ai file presenti)")
    p.add_argument("--alpha", type=float, required=True,
                   help="Alpha usato per i file trajectory_* (es. 0.01)")

    p.add_argument(
        "--min-times-observable",
        type=int,
        default=2,
        help=(
            "Criterio inclusione: clonotipo osservabile in >= N timepoint. "
            "Usa 0 per includere tutti i clonotipi (nessun filtro). "
            "Default: 2"
        ),
    )
    p.add_argument("--freq-col", type=str, default=None,
                   help="Colonna frequenza da usare (default: auto, preferisce freq_geo)")
    p.add_argument("--time-col", type=str, default="time",
                   help="Nome colonna time (default: time)")
    p.add_argument("--observable-col", type=str, default="observable",
                   help="Nome colonna observable (default: observable)")

    p.add_argument("--pairing", choices=["consecutive", "all"], default="consecutive",
                   help="Come costruire transizioni per clone: consecutive (default) o all (tutte le coppie)")
    p.add_argument("--include-non-observable", action="store_true",
                   help="Se impostato, NON filtra observable=True (sconsigliato per primo confronto)")

    p.add_argument("--ou-maxiter", type=int, default=2000,
                   help="Max iterazioni ottimizzatore OU (default: 2000)")

    p.add_argument("--refine-ou", action="store_true",
                   help="Esegue rifinitura OU per bin di frequenza (baseline per clone)")
    p.add_argument("--bin-mode", choices=["baseline", "mean"], default="baseline",
                   help="Come definire frequenza per binning: baseline=prima osservazione (default), mean=media per clone")
    p.add_argument("--bin-space", choices=["log10", "linear"], default="log10",
                   help="Spazio per binning: log10 (default) o linear")
    p.add_argument("--n-bins", type=int, default=6,
                   help="Numero di bin (se non si forniscono --bin-edges). Default 6.")
    p.add_argument("--bin-edges", type=str, default=None,
                   help="Edges personalizzati separati da virgola (es: -6,-5,-4,-3). Interpreta nello spazio scelto.")
    p.add_argument("--min-transitions-per-bin", type=int, default=100,
                   help="Minimo numero di transizioni richiesto per fittare OU in un bin (default 100)")

    p.add_argument("--bootstrap", type=int, default=0,
                   help="Numero bootstrap per CI su OU bin (default 0 = no bootstrap)")
    p.add_argument("--bootstrap-seed", type=int, default=123,
                   help="Seed bootstrap (default 123)")

    return p


def main():
    args = build_argparser().parse_args()

    if not (0 < args.alpha < 1):
        raise ValueError("alpha deve essere tra 0 e 1")
    if args.min_times_observable < 0:
        raise ValueError("--min-times-observable deve essere >= 0")

    results_dir: Path = args.results_dir
    if not results_dir.exists():
        raise FileNotFoundError(f"results-dir non trovato: {results_dir}")

    out_dir: Path = args.out_dir if args.out_dir is not None else (results_dir / "dynamics")
    ensure_dir(out_dir)

    traj_path = results_dir / f"trajectory_observability_{args.null_choice}_alpha{args.alpha}.csv"
    counts_path = results_dir / f"trajectory_observability_counts_{args.null_choice}_alpha{args.alpha}.csv"

    if not traj_path.exists():
        raise FileNotFoundError(f"Non trovo: {traj_path}")
    if not counts_path.exists():
        raise FileNotFoundError(f"Non trovo: {counts_path}")

    traj = pd.read_csv(traj_path)
    counts = pd.read_csv(counts_path)

    clone_col = infer_clone_col(traj)
    time_col = args.time_col
    observable_col = args.observable_col

    if time_col not in traj.columns:
        raise ValueError(f"time-col='{time_col}' non presente in traj. Colonne: {list(traj.columns)}")
    if observable_col not in traj.columns:
        raise ValueError(f"observable-col='{observable_col}' non presente in traj. Colonne: {list(traj.columns)}")

    # Se trajectory_* non contiene frequenze, proviamo a recuperarle dal file per_clone_denoised_* e fare merge.
    if args.freq_col is None:
        try:
            freq_col = infer_freq_col(traj)
        except ValueError:
            freq_map = load_freq_from_denoised(results_dir, args.null_choice, args.alpha)
            traj = traj.copy()
            traj = traj.rename(columns={clone_col: "clone"})
            traj = traj.merge(freq_map, on=["subject", "time", "clone"], how="left")
            traj = traj.rename(columns={"clone": clone_col})
            freq_col = "freq"
    else:
        freq_col = args.freq_col

    if freq_col not in traj.columns:
        raise ValueError(f"freq-col='{freq_col}' non presente in traj. Colonne: {list(traj.columns)}")

    # -------------------------
    # inclusion filter (supports N=0)
    # -------------------------
    if args.min_times_observable == 0:
        incl = traj[["subject", clone_col]].drop_duplicates().copy()
        incl["n_times_observable"] = np.nan
    else:
        if "n_times_observable" in counts.columns and ("subject" in counts.columns) and (clone_col in counts.columns):
            incl = counts[counts["n_times_observable"] >= args.min_times_observable].copy()
        else:
            tmp = (traj.groupby(["subject", clone_col], as_index=False)
                       .agg(n_times_observable=(observable_col, "sum")))
            incl = tmp[tmp["n_times_observable"] >= args.min_times_observable].copy()

    included_out = out_dir / f"included_clones_min{args.min_times_observable}.csv"
    incl.to_csv(included_out, index=False)

    # Filter traj to included clones (fast merge instead of apply)
    incl_keys = incl[["subject", clone_col]].copy()
    traj_f = traj.merge(incl_keys, on=["subject", clone_col], how="inner").copy()

    # Create log_freq column (always based on chosen freq_col)
    freq_vals = pd.to_numeric(traj_f[freq_col], errors="coerce").to_numpy(dtype=float)
    traj_f["log_freq"] = np.log(np.clip(freq_vals, 1e-12, None))

    traj_used_out = out_dir / f"trajectories_long_min{args.min_times_observable}.csv"
    traj_f.to_csv(traj_used_out, index=False)

    subjects = sorted(traj_f["subject"].dropna().unique().astype(int).tolist())
    gbm_rows, ou_rows, cmp_rows = [], [], []

    for sid in subjects:
        trans = extract_transitions_for_subject(
            traj=traj_f,
            subject=sid,
            clone_col=clone_col,
            freq_col=freq_col,
            time_col=time_col,
            observable_col=observable_col,
            pairing=args.pairing,
            use_only_observable=(not args.include_non_observable),
        )

        gbm = fit_gbm(trans)
        if gbm is None:
            gbm_rows.append({"subject": sid, "n_transitions": trans.n, "mu": np.nan, "sigma": np.nan, "loglik": np.nan})
        else:
            gbm_rows.append({"subject": sid, "n_transitions": gbm.n, "mu": gbm.mu, "sigma": gbm.sigma, "loglik": gbm.loglik})

        ou = fit_ou(trans, maxiter=args.ou_maxiter)
        if ou is None:
            ou_rows.append({"subject": sid, "n_transitions": trans.n, "lam": np.nan, "m": np.nan, "sigma": np.nan,
                            "loglik": np.nan, "success": False, "message": "insufficient_transitions"})
        else:
            ou_rows.append({"subject": sid, "n_transitions": ou.n, "lam": ou.lam, "m": ou.m, "sigma": ou.sigma,
                            "loglik": ou.loglik, "success": ou.success, "message": ou.message})

        if gbm is None or ou is None:
            cmp_rows.append({"subject": sid, "n_transitions": trans.n, "AIC_GBM": np.nan, "BIC_GBM": np.nan,
                             "AIC_OU": np.nan, "BIC_OU": np.nan, "deltaAIC_OU_minus_GBM": np.nan, "winner_AIC": "NA"})
        else:
            aic_gbm = aic(gbm.loglik, k=2)
            bic_gbm = bic(gbm.loglik, k=2, n=gbm.n)
            aic_ou  = aic(ou.loglik, k=3)
            bic_ou  = bic(ou.loglik, k=3, n=ou.n)
            delta = aic_ou - aic_gbm
            winner = "OU" if aic_ou < aic_gbm else "GBM"
            cmp_rows.append({
                "subject": sid, "n_transitions": trans.n,
                "AIC_GBM": aic_gbm, "BIC_GBM": bic_gbm,
                "AIC_OU": aic_ou, "BIC_OU": bic_ou,
                "deltaAIC_OU_minus_GBM": delta,
                "winner_AIC": winner
            })

    gbm_df = pd.DataFrame(gbm_rows)
    ou_df  = pd.DataFrame(ou_rows)
    cmp_df = pd.DataFrame(cmp_rows)

    gbm_out = out_dir / "gbm_fit_by_subject.csv"
    ou_out  = out_dir / "ou_fit_by_subject.csv"
    cmp_out = out_dir / "model_comparison_by_subject.csv"

    gbm_df.to_csv(gbm_out, index=False)
    ou_df.to_csv(ou_out, index=False)
    cmp_df.to_csv(cmp_out, index=False)

    print("Salvati:")
    print(" -", included_out)
    print(" -", traj_used_out)
    print(" -", gbm_out)
    print(" -", ou_out)
    print(" -", cmp_out)

    # -------------------------
    # Optional: OU refinement by frequency bins
    # -------------------------
    if args.refine_ou:
        refined_rows = []
        ci_rows = []

        edges = None
        if args.bin_edges:
            edges = [float(x.strip()) for x in args.bin_edges.split(",") if x.strip()]
            if len(edges) < 2:
                raise ValueError("--bin-edges deve avere almeno due valori")

        for sid in subjects:
            df_s = traj_f[traj_f["subject"] == sid].copy()

            # apply observable filter unless include_non_observable
            if not args.include_non_observable:
                df_s = df_s[df_s[observable_col] == True].copy()

            df_s = df_s.dropna(subset=[freq_col, time_col, clone_col])
            df_s[freq_col] = pd.to_numeric(df_s[freq_col], errors="coerce")
            df_s = df_s.dropna(subset=[freq_col])
            df_s = df_s[df_s[freq_col] > 0].copy()
            if df_s.empty:
                continue

            df_s = df_s.sort_values([clone_col, time_col])

            # define per-clone value used for bin assignment
            if args.bin_mode == "baseline":
                f_clone = (df_s.groupby(clone_col, as_index=False).first()[[clone_col, freq_col]]
                             .rename(columns={freq_col: "f_bin"}))
            else:
                f_clone = (df_s.groupby(clone_col, as_index=False)[freq_col].mean()
                             .rename(columns={freq_col: "f_bin"}))

            vals = f_clone["f_bin"].to_numpy(dtype=float)
            if args.bin_space == "log10":
                vals = np.log10(np.clip(vals, 1e-12, None))

            f_clone["bin"] = assign_bins(vals, n_bins=args.n_bins, edges=edges)
            bin_map = dict(zip(f_clone[clone_col].astype(str), f_clone["bin"].astype(int)))

            dts, x0s, x1s, bins = [], [], [], []
            for cid, g in df_s.groupby(clone_col):
                t = pd.to_numeric(g[time_col], errors="coerce").to_numpy(dtype=float)
                f = g[freq_col].to_numpy(dtype=float)
                if t.size < 2:
                    continue
                x = safe_log(f)
                b = bin_map.get(str(cid))
                if b is None:
                    continue

                if args.pairing == "consecutive":
                    for i in range(t.size - 1):
                        dt = float(t[i + 1] - t[i])
                        if dt <= 0:
                            continue
                        dts.append(dt); x0s.append(float(x[i])); x1s.append(float(x[i + 1])); bins.append(int(b))
                else:
                    for i, j in all_pairs_indices(t.size):
                        dt = float(t[j] - t[i])
                        if dt <= 0:
                            continue
                        dts.append(dt); x0s.append(float(x[i])); x1s.append(float(x[j])); bins.append(int(b))

            if not dts:
                continue

            dts = np.asarray(dts, dtype=float)
            x0s = np.asarray(x0s, dtype=float)
            x1s = np.asarray(x1s, dtype=float)
            bins = np.asarray(bins, dtype=int)

            for b in sorted(np.unique(bins).tolist()):
                mask = bins == b
                ntr = int(mask.sum())

                if ntr < args.min_transitions_per_bin:
                    refined_rows.append({
                        "subject": sid, "bin": b, "n_transitions": ntr,
                        "lam": np.nan, "m": np.nan, "sigma": np.nan, "loglik": np.nan,
                        "success": False, "message": f"too_few_transitions(<{args.min_transitions_per_bin})"
                    })
                    continue

                trans_b = TransitionSet(dt=dts[mask], x0=x0s[mask], x1=x1s[mask], subject=sid)
                fitb = fit_ou(trans_b, maxiter=args.ou_maxiter)
                if fitb is None:
                    refined_rows.append({
                        "subject": sid, "bin": b, "n_transitions": ntr,
                        "lam": np.nan, "m": np.nan, "sigma": np.nan, "loglik": np.nan,
                        "success": False, "message": "fit_failed"
                    })
                    continue

                refined_rows.append({
                    "subject": sid, "bin": b, "n_transitions": ntr,
                    "lam": fitb.lam, "m": fitb.m, "sigma": fitb.sigma, "loglik": fitb.loglik,
                    "success": fitb.success, "message": fitb.message
                })

                if args.bootstrap and args.bootstrap > 0 and fitb.success:
                    boot = bootstrap_ou(
                        x0s[mask], x1s[mask], dts[mask],
                        n_boot=args.bootstrap, maxiter=args.ou_maxiter,
                        seed=args.bootstrap_seed + sid * 1000 + b
                    )
                    if not boot.empty:
                        ci_rows.append({
                            "subject": sid, "bin": b, "n_boot": int(boot.shape[0]),
                            "lam_p2.5": float(np.quantile(boot["lam"], 0.025)),
                            "lam_p50": float(np.quantile(boot["lam"], 0.5)),
                            "lam_p97.5": float(np.quantile(boot["lam"], 0.975)),
                            "m_p2.5": float(np.quantile(boot["m"], 0.025)),
                            "m_p50": float(np.quantile(boot["m"], 0.5)),
                            "m_p97.5": float(np.quantile(boot["m"], 0.975)),
                            "sigma_p2.5": float(np.quantile(boot["sigma"], 0.025)),
                            "sigma_p50": float(np.quantile(boot["sigma"], 0.5)),
                            "sigma_p97.5": float(np.quantile(boot["sigma"], 0.975)),
                        })

        refined_df = pd.DataFrame(refined_rows)
        refined_out = out_dir / "ou_refined_by_subject_bin.csv"
        refined_df.to_csv(refined_out, index=False)
        print(" -", refined_out)

        if ci_rows:
            ci_df = pd.DataFrame(ci_rows)
            ci_out = out_dir / "ou_refined_by_subject_bin_bootstrap_ci.csv"
            ci_df.to_csv(ci_out, index=False)
            print(" -", ci_out)

    print("\nDONE.")


if __name__ == "__main__":
    main()
